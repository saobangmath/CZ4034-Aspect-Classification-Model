data:
    train:
        paths:
            - data/train.csv
    val:
        paths:
            - data/validation.csv
    test:
        paths:
            - data/test.csv

model:
    model_name_or_path: bert-base-cased
    config_name: null
    tokenizer_name: null
    cache_dir: null
    cls_token: [CLS]  # need this to map token index. might be "<s>" or "[CLS]", depending on the tokenizer
    sep_token: [CLS]  # need this to map token index. might be "</s>" or "[SEP]", depending on the tokenizer

    model_class: BertForReviewAspectClassification
    freeze_base_model: false
    fusion: max_pooling
    lambdas: [1, 1, 1, 1, 1, 1]
    mean: 0.5
    std: 3

training:
    work_dir: work_dirs/  # set to `null` to not save anything
    learning_rate: 5e-5
    weight_decay: 0.01
    lr_warmup: 0.1  # fraction of total number of iterations used to warm up training
    max_grad_norm: 1.0

    device: cpu
    batch_size: 6
    batch_size_multiplier: 1.5  # eval/train batch size ratio
    num_epochs: 100
    num_workers: 2
    debugging: false  # set to 'true' to perform only 10 iterations per epoch
    early_stopping: 2  # stop training when model is not improved over this number of epochs

evaluation:
    confidence_threshold: 0.51  # predictions with confidence thresholds less than this will be discarded
